{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccda3bab-08ef-466f-9ba8-b81b62d826c4",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905574bf-08ad-4329-b4e5-b6afd110747c",
   "metadata": {},
   "source": [
    "## Spliting the data / Hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72957b2a-4e3b-4ecd-b603-15a27fddbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would need these libraries to manage our dataset\n",
    "# Numpy: used for large, multi-dimensional arrays and matrices, and for high-level mathematical functions\n",
    "# Pandas: used for data manipulation and analysis\n",
    "# matplotlib: used for visualisation and plotting graph/image/etc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ca4066-9650-4b34-b1e0-560ef6411dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the iris dataset from sklearn\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "from sklearn.datasets import load_iris\n",
    "# load the dataset\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48394239-f68d-4ca5-9925-dd472f8c4ad6",
   "metadata": {},
   "source": [
    "For the sake of the example, we use multiple model for classification. We don't need to know how they works, just that they have a `fit` and a `predict` method, like all model on scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e36fee-d7c7-413b-a06e-475092a5bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import differents classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3dba4-09b8-4ff6-a828-55d78e8ce218",
   "metadata": {},
   "source": [
    "scikit-learn provides many function for model selection and dataset management, the most simple being `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588ada22-1d96-4beb-b38e-b3087fbf77b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2) (38, 2) (array([0, 1, 2]), array([35, 39, 38]))\n"
     ]
    }
   ],
   "source": [
    "# import the function for splitting from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(RANDOM_SEED)\n",
    "X = iris.data[:, :2] # .reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, iris.target)\n",
    "print(X_train.shape, X_test.shape, np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fc7f0-7411-4636-a1ad-7fab6c836e2e",
   "metadata": {},
   "source": [
    "We want a training set, a testing set and a validation set, so we have to do 2 splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609d8a07-cf1f-431f-91da-930545f910f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_vali, y_training, y_vali = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fd224-7fab-4684-bb0b-cf09433912a9",
   "metadata": {},
   "source": [
    "We create and train our model on the training set  \n",
    "\\- don't mind the warning, it just means the neural network have not converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96553b30-ec85-429d-bf4a-e08543805f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackred/anaconda3/envs/pDL/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier().fit(X_training, y_training)\n",
    "mlp_clf = MLPClassifier().fit(X_training, y_training)\n",
    "svc_clf = SVC().fit(X_training, y_training)\n",
    "knn_clf = KNeighborsClassifier().fit(X_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390b9e3-e087-4857-8e46-540ad1050d73",
   "metadata": {},
   "source": [
    "After the training, we test our model on the validation set, and check which one is the most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820783e4-8cc5-4b18-816b-e208703ae57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res = rf_clf.predict(X_vali)\n",
    "mlp_res = mlp_clf.predict(X_vali)\n",
    "svc_res = svc_clf.predict(X_vali)\n",
    "knn_res = knn_clf.predict(X_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a83322-0d38-469a-8b23-9a41bb2c29fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF  0.75\n",
      "MLP 0.6428571428571429\n",
      "SVC 0.7857142857142857\n",
      "KNN 0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "# scikit learn has many metrics function\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('RF ', accuracy_score(rf_res, y_vali))\n",
    "print('MLP', accuracy_score(mlp_res, y_vali))\n",
    "print('SVC', accuracy_score(svc_res, y_vali))\n",
    "print('KNN', accuracy_score(knn_res, y_vali))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975a548-85e4-498e-bec4-d767e155cb3f",
   "metadata": {},
   "source": [
    "Since KNN is the model with the best accuracy, we chose it. We will now evaluate it on the testing set, which have not been used before neither.\n",
    "\n",
    "Usually, the training-validation phase would be run multiple time, with different hyperparameter/model/etc, and the testing would be the final part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92e8f9f-91de-4d5a-a93d-42e1ba924d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "test_res = knn_clf.predict(X_test)\n",
    "print('KNN', accuracy_score(test_res, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502febf-a4d4-4ea2-87fc-ee7524ecb7f9",
   "metadata": {},
   "source": [
    "## Hyperparameter selection\n",
    "\n",
    "scikit-learn provided many hyperparameter selection object, the simplest and most common being the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e7b4f8-c571-4723-9040-b0b3ce79844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 4, 'n_estimators': 30} 0.758102766798419\n"
     ]
    }
   ],
   "source": [
    "# import the grid search from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# create a classifier\n",
    "rf_grid = RandomForestClassifier()\n",
    "# define the list of hyperparameter we eant to evaluate\n",
    "# as well as the range of value to test for each one\n",
    "parameters = {'n_estimators': [2, 5, 10, 15, 25, 30, 50], 'min_samples_split': range(2,7)}\n",
    "# create the grid search object\n",
    "# it behaves like a sklearn model, with the fit and predict method\n",
    "grid = GridSearchCV(rf_grid, parameters)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3f299-7135-43af-a51e-8b9d7ff8f30c",
   "metadata": {},
   "source": [
    "Once the grid search object is trained, using the predict function will automatically use the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25b97cf-66d7-4931-bc1c-2d8d176b1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF  0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "grid_res = grid.predict(X_test)\n",
    "print('RF ', accuracy_score(grid_res, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fc4d2-e84d-4939-bd6f-b5a37a9e566a",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "cross-validation with scikit-learn is yet another different way to do things than hold out or grid search. You call a function (`cross-validate`) which you give the model as argument. \n",
    "\n",
    "We are only a training set which will be split during the CV. We can use a testing set after selecting the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9994498-4e75-4e5c-a4c1-a85dba14e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF  [0.78571429 0.67857143 0.67857143 0.67857143]\n",
      "0.705357142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "# a classifier\n",
    "rf_cv = RandomForestClassifier(n_estimators=30, min_samples_split=4)\n",
    "# cv is the type of cross-validation\n",
    "# if you give an int as argument, it is the number of fold you want for\n",
    "# k-fold cross-validation\n",
    "cv_res = cross_validate(rf_cv, X_train, y_train, cv=4)\n",
    "# the return value of the cross-validation is the list of the score obtained by each\n",
    "# model during the cross-validation\n",
    "print('RF ', cv_res['test_score'])\n",
    "print(np.mean(cv_res['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0a7b0-82b0-4305-b237-f906218bccc2",
   "metadata": {},
   "source": [
    "Leave-one-out cross-validation is yet another way do split dataset in scikit-learn. You create a splitter object, which will gives you a list of the indexes of the element of each group. There is other splitter object in scikit-learn that works the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229fe1f9-5d89-45f7-b97a-84a4240401ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 112\n",
      "(array([0., 1.]), array([36, 76])) average 0.6785714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "# a classifier\n",
    "rf_lpo = RandomForestClassifier(n_estimators=30, min_samples_split=4)\n",
    "# the splitter\n",
    "lpo = LeavePOut(1)\n",
    "# displaying how much split we have\n",
    "n_split = lpo.get_n_splits(X_train)\n",
    "print('split', n_split)\n",
    "lpo_res = []\n",
    "j = 0\n",
    "# the lpo.split function generate a list that we have to iterate through\n",
    "# each element of the list has 2 elements, the indexes of the element to use for training\n",
    "# the indexes of the element to use for testing\n",
    "for train_idx, test_idx in lpo.split(X_train):\n",
    "    print(j+1, end='\\r')\n",
    "    j = j+1\n",
    "    rf_lpo.fit(X_train[train_idx], y_train[train_idx])\n",
    "    lpo_res.append(accuracy_score(y_train[test_idx], rf_lpo.predict(X_train[test_idx])))\n",
    "print(np.unique(lpo_res, return_counts=True), 'average', np.mean(lpo_res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
